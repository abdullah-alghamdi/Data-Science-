{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle's San Francisco Crime Classification,  \n",
    "Abdullah Alghamdi \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, \\\n",
    "ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.decomposition import PCA\n",
    "from copy import deepcopy\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/Users/hamrani/Documents/datascience/sf/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions to parse date and season to analyze the time of the incedint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions for time and season:\n",
    "\n",
    "def get_time(x):\n",
    "    date=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
    "    time=date.hour\n",
    "    day=date.day\n",
    "    month=date.month\n",
    "    year=date.year\n",
    "    return time,day,month,year\n",
    "\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    get_data function drops columns that we dont need, and add additional columns ( feature engineering )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data1,odd,oddp):\n",
    "    feature=data1.columns.tolist()\n",
    "    if \"Descript\" in feature:\n",
    "        feature.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature:\n",
    "        feature.remove(\"Resolution\")\n",
    "    if \"Category\" in feature:\n",
    "        feature.remove(\"Category\")\n",
    "    if \"Id\" in feature:\n",
    "        feature.remove(\"Id\")\n",
    "    cData=data1[feature]\n",
    "    cData.index=range(len(data1))\n",
    "    address_features=cData[\"Address\"].apply(lambda x: odd[x])\n",
    "    address_features.columns=[\"odd\"+str(x) for x in range(len(address_features.columns))]\n",
    "    cData[\"Time\"], cData[\"Day\"], cData[\"Month\"], cData[\"Year\"]=zip(*cData[\"Dates\"].apply(get_time))\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    dummy_PD = pd.get_dummies(cData['PdDistrict'], prefix='PD')\n",
    "    dummy_DAY = pd.get_dummies(cData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cData[\"oddp\"]=cData[\"Address\"].apply(lambda x: oddp[x])\n",
    "    cData=cData.drop(\"PdDistrict\",axis=1)\n",
    "    cData=cData.drop(\"DayOfWeek\",axis=1)\n",
    "    cData=cData.drop(\"Address\",axis=1)\n",
    "    cData=cData.drop(\"Dates\",axis=1)\n",
    "    feature =cData.columns.tolist()\n",
    "    features = cData[feature].join(dummy_PD.ix[:,:]).join(dummy_DAY.ix[:,:]).join(address_features.ix[:,:])\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in data1.columns:\n",
    "        categ= data1[\"Category\"].astype('category')\n",
    "    else:\n",
    "        categ=None\n",
    "    return features, categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important part, where we apply the logit ( or log_loss) to the addresses column. \n",
    "https://en.wikipedia.org/wiki/Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses=sorted(data[\"Address\"].unique())\n",
    "categories=sorted(data[\"Category\"].unique())\n",
    "c_num=data.groupby([\"Category\"]).size()\n",
    "ac_num=data.groupby([\"Address\",\"Category\"]).size()\n",
    "a_num=data.groupby([\"Address\"]).size()\n",
    "odd={}\n",
    "oddp={}\n",
    "minn=2\n",
    "def_log =np.log(c_num/len(data))-np.log(1.0-c_num/float(len(data)))\n",
    "for addr in addresses:\n",
    "    p =a_num[addr]/float(len(data))\n",
    "    oddp[addr]=np.log(p)-np.log(1.-p)\n",
    "    odd[addr]=deepcopy(def_log)\n",
    "    for cat in ac_num[addr].keys():\n",
    "        if (ac_num[addr][cat]>minn) and ac_num[addr][cat]<a_num[addr]:\n",
    "            p=ac_num[addr][cat]/float(a_num[addr])\n",
    "            odd[addr][categories.index(cat)]=np.log(p)-np.log(1.0-p)\n",
    "    odd[addr]=pd.Series(odd[addr])\n",
    "    odd[addr].index=range(len(categories))\n",
    "    \n",
    "features, categ=get_data(data,odd,oddp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added some PCA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca=PCA(n_components=60)\n",
    "pca.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = features\n",
    "y = categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression works well with such a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"logloss\", log_loss(categ, model.predict_proba(features.as_matrix()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the test dataset and finish the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"/Users/hamrani/Documents/datascience/sf/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_addresses = sorted(test[\"Address\"].unique())\n",
    "test_a_num = test.groupby(\"Address\").size()\n",
    "test_addr=set(test_addresses+addresses)-set(addresses)\n",
    "train_addr=set(test_addresses+addresses)-set(test_addresses)\n",
    "both=set(new_addresses).intersection(addresses)\n",
    "for addr in test_addr:\n",
    "    p = test_a_num[addr]/float(len(test)+len(data))\n",
    "    oddp[addr]=np.log(p)-np.log(1.-p)\n",
    "    odd[addr]=deepcopy(def_log)\n",
    "    odd[addr].index=range(len(categories))\n",
    "for addr in both:\n",
    "    p =(a_num[addr]+test_num[addr])/float(len(test)+len(data))\n",
    "    oddp[addr]=np.log(p)-np.log(1.-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_after, _= get_data(test, odd, oddp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scaled the features here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler=preprocessing.StandardScaler()\n",
    "col = features_after.columns.tolist()\n",
    "features_after[col]=scaler.transform(features_after[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction=pd.DataFrame(model.predict_proba(features_after.as_matrix()),columns=sorted(categ.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction.to_csv(\"abdullah_r1.csv\",index_label=\"Id\",na_rep=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
